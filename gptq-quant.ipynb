{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio transformers vllm -y\n",
    "!pip cache purge\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install --upgrade transformers vllm datasets tqdm \n",
    "!pip install -U gptqmodel --no-build-isolation -v\n",
    "!pip install optimum\n",
    "!huggingface-cli login --token **    # Read\n",
    "# !huggingface-cli login --token **    # Write\n",
    "!rm -rf /kaggle/working/your_folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "def evaluate_ppl(model, tokenizer, device=\"cuda:0\"):\n",
    "    model.to(device)\n",
    "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    \n",
    "    test_enc = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    model.seqlen = 2048\n",
    "    test_enc = test_enc.input_ids.to(device)\n",
    "    \n",
    "    nsamples = test_enc.numel() // model.seqlen\n",
    "    nlls = []  \n",
    "    for i in tqdm(range(nsamples), desc=\"Evaluating PPL...\"):\n",
    "        batch = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * model.seqlen\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "    \n",
    "    return ppl.item()\n",
    "\n",
    "\n",
    "def main():\n",
    "    ############## Set Up ##############\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    max_new_tokens = 256    # Number of new tokens to generate\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    ### === Load model with BitsAndBytes w4a16 quantization ===\n",
    "    model_name = \"zbyzby/Llama3.2-3B-Instruct-QLoRA-finetuned\"\n",
    "    folder_path = \"/kaggle/working/your_folder\" \n",
    "\n",
    "    calibration_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(1024))[\"text\"]\n",
    "    quant_config = QuantizeConfig(bits=4, group_size=128)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = GPTQModel.load(model_name)\n",
    "\n",
    "    # Quant\n",
    "    model.quantize(calibration_dataset, batch_size=2)\n",
    "\n",
    "    model.save(folder_path)\n",
    "    tokenizer.save_pretrained(folder_path)\n",
    "\n",
    "    from huggingface_hub import HfApi\n",
    "    import os\n",
    "    HF_TOKEN = \"**\" # Replace with your Hugging Face token\n",
    "    repo_id = \"zbyzby/Llama3.2-3B-Instruct-quantized\"\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            path_in_repo = os.path.relpath(file_path, folder_path)\n",
    "            print(f\"Uploading: {file_path} -> {path_in_repo}\")\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=file_path,\n",
    "                path_in_repo=path_in_repo,\n",
    "                repo_id=repo_id,\n",
    "                token=HF_TOKEN,\n",
    "            )\n",
    "    print(\"All files uploaded.\")\n",
    "\n",
    "    # Test perplexity\n",
    "    print(\"\\n=== Testing Perplexity ===\")\n",
    "    ppl = evaluate_ppl(model, tokenizer, device)\n",
    "    print(f\"Perplexity (PPL): {ppl}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "quant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\"\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\"\n",
    ").select(range(1024))[\"text\"]\n",
    "quant_config = QuantizeConfig(bits=4, group_size=128)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = GPTQModel.load(model_id, quant_config)\n",
    "\n",
    "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
    "model.quantize(calibration_dataset, batch_size=2)\n",
    "\n",
    "model.save(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "HF_TOKEN = \"**\" # Replace with your Hugging Face token\n",
    "repo_id = \"zbyzby/Llama-3.2-1B-Instruct-GPTQ-Quant\"\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "for root, dirs, files in os.walk(quant_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        path_in_repo = os.path.relpath(file_path, quant_path)\n",
    "        print(f\"Uploading: {file_path} -> {path_in_repo}\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=repo_id,\n",
    "            token=HF_TOKEN,\n",
    "        )\n",
    "print(\"All files uploaded.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
